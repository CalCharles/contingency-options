python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xstatestab_1/ --optim base --reward-form dense > outtab_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xstatestile10_1/ --optim base --reward-form dense > outtile10_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 30 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xstatestile30_1/ --optim base --reward-form dense > outtile30_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxtab_1/ --optim base --reward-form dense > outtabp_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor  --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxtile10_1/ --optim base --reward-form dense > outtile10p_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxtile20_1/ --optim base --reward-form dense > outtile20p_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxbotab_1/ --optim base --reward-form dense > outtab_pb1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxbotile10_1/ --optim base --reward-form dense > outtilepb10_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/proxbotile20_1/ --optim base --reward-form dense > outtilepb20_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xpxbotab_1/ --optim base --reward-form dense > outtabxb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xpxbotile10_1/ --optim base --reward-form dense > outtile10xb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/xpxbotile20_1/ --optim base --reward-form dense > outtile20xb_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms bounds bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/bbtab_1/ --optim base --reward-form dense > outtabbb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms bounds bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/bbtile10_1/ --optim base --reward-form dense > outtile10bb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms bounds bounds --state-names Paddle ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/bbtile20_1/ --optim base --reward-form dense > outtile20bb_1.txt


python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 7 --save-dir datasets/caleb_data/cotest/xstatesPPOf64ge7_1/ --optim Adam > outPPOf64ge7_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 7 --save-dir datasets/caleb_data/cotest/xstatesPPOf2ge7_1/ --optim Adam > outPPOf2ge7_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 10 --save-dir datasets/caleb_data/cotest/xstatesPPOf64ge7_1/ --optim Adam > outPPOf64ge10_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 10 --save-dir datasets/caleb_data/cotest/xstatesPPOf2ge7_1/ --optim Adam > outPPOf2ge10_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 7 --save-dir datasets/caleb_data/cotest/xstatesPPOf64ge7_2/ --optim Adam > outPPOf64ge7_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 7 --save-dir datasets/caleb_data/cotest/xstatesPPOf2ge7_2/ --optim Adam > outPPOf2ge7_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 10 --save-dir datasets/caleb_data/cotest/xstatesPPOf64ge7_2/ --optim Adam > outPPOf64ge10_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 10 --save-dir datasets/caleb_data/cotest/xstatesPPOf2ge7_2/ --optim Adam > outPPOf2ge10_2.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb1_1/ --optim Adam > outPPOf64eb1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb1_1/ --optim Adam > outPPOf2eb1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb01_1/ --optim Adam > outPPOf64eb01_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb01_1/ --optim Adam > outPPOf2eb01_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb1_2/ --optim Adam > outPPOf64eb1_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb1_2/ --optim Adam > outPPOf2eb1_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb01_2/ --optim Adam > outPPOf64eb01_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb01_2/ --optim Adam > outPPOf2eb01_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb1_3/ --optim Adam > outPPOf64eb1_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .1 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb1_3/ --optim Adam > outPPOf2eb1_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf64eb01_3/ --optim Adam > outPPOf64eb01_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --dist-interval 3 --exp-beta .01 --save-dir datasets/caleb_data/cotest/xstatesPPOf2eb01_3/ --optim Adam > outPPOf2eb01_3.txt


python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64rewx_1/ --optim Adam > outPPOf64rewx_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2rewx_1/ --optim Adam > outPPOf2rewx_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64rewx_2/ --optim Adam > outPPOf64rewx_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2rewx_2/ --optim Adam > outPPOf2rewx_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64rewx_3/ --optim Adam > outPPOf64rewx_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2rewx_3/ --optim Adam > outPPOf2rewx_3.txt
