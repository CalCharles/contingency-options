python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtab_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile10_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 30 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile30_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtabp_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor  --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile10p_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile20p_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtab_pb1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtilepb10_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtilepb20_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtabxb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile10xb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile20xb_1.txt
python paddle_bounce.py --model-form tab --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms bounds bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtabbb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms bounds bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 10 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile10bb_1.txt
python paddle_bounce.py --model-form tile --optimizer-form TabQ --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms bounds bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 20 --num-layers 2 --greedy-epsilon .2 --lr .005 --normalize --behavior-policy egq --optim base --reward-form bounce > outtile20bb_1.txt


# python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 7 --optim Adam > outPPOf64ge7_1.txt
# python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 7 --optim Adam > outPPOf2ge7_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 10 --optim Adam > outPPOf64ge10_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 10 --optim Adam > outPPOf2ge10_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 7 --optim Adam > outPPOf64ge7_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 7 --optim Adam > outPPOf2ge7_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 10 --optim Adam > outPPOf64ge10_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 10 --optim Adam > outPPOf2ge10_2.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf64eb1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf2eb1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf64eb01_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf2eb01_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf64eb1_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf2eb1_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf64eb01_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf2eb01_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf64eb1_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .1 --optim Adam > outPPOf2eb1_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf64eb01_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --dist-interval 3 --exp-beta .01 --optim Adam > outPPOf2eb01_3.txt


python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf64rewx_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf2rewx_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf64rewx_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf2rewx_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf64rewx_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 40000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form x --grad-epoch 4 --optim Adam > outPPOf2rewx_3.txt
