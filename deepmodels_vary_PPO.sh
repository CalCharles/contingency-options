python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f8_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f8_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f2_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f8_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl2f2_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl0_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl0_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOnl0_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64lre3_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2lre3_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .01 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64lre2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .01 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2lre2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .1 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64lre1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .1 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2lre1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .00001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64lre5_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .00001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2lre5_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .000001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64lre6_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .000001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2lre6_1.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf64buff_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf2buff_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf64buff_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf2buff_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf64buff_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf2buff_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf64buff_4.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --optim Adam > outPPOf2buff_4.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64non_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2non_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64non_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2non_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf64non_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form bounce --grad-epoch 4 --optim Adam > outPPOf2non_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf64bounce_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf2bounce_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf64bounce_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf2bounce_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf64bounce_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --optim Adam > outPPOf2bounce_3.txt


python paddle_bounce.py --model-form fourier --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period 1 --reward-form bounce > outfop1px_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period .05 --reward-form bounce > outgaupp5px_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period .1 --reward-form bounce > outgaup1px_2.txt
python paddle_bounce.py --model-form fourier --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period 1 --reward-form bounce > outfop1pxb_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period .05 --reward-form bounce > outgaupp5pxb_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 10000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir ../datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --optim base --period .1 --reward-form bounce > outgaup1pxb_2.txt
