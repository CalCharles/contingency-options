python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f8_1/ --optim Adam > outPPOnl2f8_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f2_1/ --optim Adam > outPPOnl2f2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f8_2/ --optim Adam > outPPOnl2f8_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f2_2/ --optim Adam > outPPOnl2f2_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f8_3/ --optim Adam > outPPOnl2f8_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 2 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl2f2_3/ --optim Adam > outPPOnl2f2_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl0_1/ --optim Adam > outPPOnl0_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl0_2/ --optim Adam > outPPOnl0_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 8 --num-layers 0 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOnl0_3/ --optim Adam > outPPOnl0_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64lre3_1/ --optim Adam > outPPOf64lre3_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2lre3_1/ --optim Adam > outPPOf2lre3_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .01 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64lre2_1/ --optim Adam > outPPOf64lre2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .01 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2lre2_1/ --optim Adam > outPPOf2lre2_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .1 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64lre1_1/ --optim Adam > outPPOf64lre1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .1 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2lre1_1/ --optim Adam > outPPOf2lre1_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .00001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64lre5_1/ --optim Adam > outPPOf64lre5_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .00001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2lre5_1/ --optim Adam > outPPOf2lre5_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .000001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64lre6_1/ --optim Adam > outPPOf64lre6_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .000001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2lre6_1/ --optim Adam > outPPOf2lre6_1.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf64buff_1/ --optim Adam > outPPOf64buff_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf2buff_1/ --optim Adam > outPPOf2buff_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf64buff_2/ --optim Adam > outPPOf64buff_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf2buff_2/ --optim Adam > outPPOf2buff_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf64buff_3/ --optim Adam > outPPOf64buff_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf2buff_3/ --optim Adam > outPPOf2buff_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf64buff_4/ --optim Adam > outPPOf64buff_4.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form dense --grad-epoch 4 --buffer-steps 10000 --num-grad-states 5 --save-dir datasets/caleb_data/cotest/xstatesPPOf2buff_4/ --optim Adam > outPPOf2buff_4.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64non_1/ --optim Adam > outPPOf64non_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2non_1/ --optim Adam > outPPOf2non_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64non_2/ --optim Adam > outPPOf64non_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2non_2/ --optim Adam > outPPOf2non_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64non_3/ --optim Adam > outPPOf64non_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --behavior-policy esp --reward-form dense --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2non_3/ --optim Adam > outPPOf2non_3.txt

python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64bounce_1/ --optim Adam > outPPOf64bounce_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2bounce_1/ --optim Adam > outPPOf2bounce_1.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64bounce_2/ --optim Adam > outPPOf64bounce_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2bounce_2/ --optim Adam > outPPOf2bounce_2.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 64 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf64bounce_3/ --optim Adam > outPPOf64bounce_3.txt
python paddle_bounce.py --model-form basic --optimizer-form PPO --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms xprox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraph --factor 2 --num-layers 1 --greedy-epsilon .2 --lr .0001 --normalize --behavior-policy esp --reward-form bounce --grad-epoch 4 --save-dir datasets/caleb_data/cotest/xstatesPPOf2bounce_3/ --optim Adam > outPPOf2bounce_3.txt


python paddle_bounce.py --model-form fourier --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/fop1px_2/ --optim base --period 1 --reward-form dense > outfop1px_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/gaupp5px_2/ --optim base --period .05 --reward-form dense > outgaupp5px_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox --state-names Paddle --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/gaup1px_2/ --optim base --period .1 --reward-form dense > outgaup1px_2.txt
python paddle_bounce.py --model-form fourier --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotes2/fop1pxb_1/ --optim base --period 1 --reward-form dense > outfop1pxb_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/gaupp5pxb_2/ --optim base --period .05 --reward-form dense > outgaupp5pxb_2.txt
python paddle_bounce.py --model-form gaussian --optimizer-form SARSA --record-rollouts "data/action/" --train-edge "Paddle->Ball" --num-stack 2 --train --num-iters 100000 --state-forms prox bounds --state-names Paddle Ball --base-node Paddle --changepoint-dir datasets/caleb_data/cotest/paddlegraphpg --factor 40 --num-layers 2 --greedy-epsilon .2 --lr .0007 --normalize --behavior-policy egq --save-dir datasets/caleb_data/cotest/gaup1pxb_2/ --optim base --period .1 --reward-form dense > outgaup1pxb_2.txt
